# HUAI_BAINSA_MDS_Challenge_2025


Predict survival and vital status in synthetic Myelodysplastic Syndromes (MDS) patients using clinical, cytogenetic and genomic data.

---

## 1. Overview

This repository contains all the material for the **HUAI Ã— BAINSA 2025 MDS Challenge**.

Participants will work with a **synthetic dataset of MDS patients** to:

1. **Predict overall survival (months)**  
2. **Predict vital status at last follow-up** (alive vs dead)

You will have **a few hours** to explore the data, build a model, and present your approach. The challenge is designed to reward not only raw performance, but also **good reasoning, clear communication, explainability, and smart collaboration between medical and technical profiles**.

- **Hosts**: HUAI & BAINSA  
- **Disease area**: Myelodysplastic Syndromes (MDS)  
- **Data**: Synthetic MDS cohort generated by Train S.r.l. based on a model trained on a European hematology registry  
- **Deadline**: All deliverables must be submitted **by 17:45** (CET)

---

## 2. Repository Structure

At the top level (main branch), you will find:

```text
.
â”œâ”€â”€ 00_Intro.pptx     # Intro presentation (challenge, rules, EDA hints)
â”œâ”€â”€ deliverable/
â”‚   â”œâ”€â”€ 01_template.pptx      # PPT template for your 3â€“5 minute presentation
â”‚   â””â”€â”€ 02_submission.xlsx             # Excel template: PatientID, survival_month, alive_dead_status
â””â”€â”€ data/
    â”œâ”€â”€ train_MDS.xlsx                               # 1000 synthetic MDS patients (with targets)
    â””â”€â”€ test_MDS.xlsx                                # 200 synthetic MDS patients (targets hidden)
````


### 2.1 Introductory PPTX (main folder)

The **introduction PPTX** includes:

* Context about MDS and the challenge
* Explanation of the prediction task
* Key **rules and deadlines**
* Quick **EDA pointers and insights** to help you start fast (given the limited time)

You are encouraged to review this file as your first step.

### 2.2 `deliverable/` folder

Contains everything you need to structure your final submission:

1. **Presentation template (PPTX)**

   * Use this for your **3â€“5 minute team presentation**.
   * Suggested sections:

     * Problem understanding
     * Personalized Data exploration summary
     * Modeling approach
     * Key reasoning
     * Explainability & insights


2. **Submission Excel template**

   * This is the file you will fill in and send as your **prediction deliverable**.

   * It must contain exactly three columns:

     | Column name         | Description                                                       |
     | ------------------- | ----------------------------------------------------------------- |
     | `PatientID`         | Patient ID from the **test set**                                  |
     | `survival_month`    | Predicted overall survival time (in months)                       |
     | `alive_dead_status` | Predicted vital status: **1 = alive, 2 = dead** at last follow-up |

   * All **PatientID values must match exactly** those in the `test` data.

### 2.3 `data/` folder

Contains:

* **`train` dataset**

  * ~1000 synthetic MDS patients
  * Includes:

    * All predictors (features)
    * Outcome variables (targets)
* **`test` dataset**

  * ~200 synthetic MDS patients
  * Includes:

    * Same predictors as `train`
    * **Target columns are masked/empty** (these are what you must predict)

---

## 3. Data Description

Both **train** and **test** share the same feature set. The data was generated using a **synthetic model by Train S.r.l.**, trained on a European hematology registry. This means:

* No real patient-level data is exposed
* Relationships between variables aim to mimic realistic clinical patterns

Below is a high-level description of the **main feature groups**.

### 3.1 Identifiers

* **`Patient ID`**
  Unique identifier for each (synthetic) patient.

  * **Do not** use this for modeling (except to merge or index). You can assume rows are independent. Remember ID purpose in test set.
  * It must be preserved in the final submission.

### 3.2 Demographic & Baseline Clinical Variables

Typical columns:

* **`Age at diagnosis (years)`**: age when MDS was diagnosed
* **`Gender (M=1/F=2)`**: 1 = male, 2 = female
* **`WHO2016 Class`**: categorical label describing the MDS subtype according to WHO 2016
* **`Hemoglobin (g/L)`**: baseline hemoglobin level
* **`Neutrophils (10 9/L)`**: neutrophil count
* **`Platelets (10 9/L)`**: platelet count
* **`Bone Marrow Blasts (%)`**: percentage of blasts in bone marrow

These are core clinical descriptors and generally strong predictors of prognosis in MDS.

### 3.3 Risk Scores & Outcome Variables (Train Only)

#### Prognostic scores

* **`IPSSR Risk Group`**

  * Overall risk category according to the IPSS-R scoring system (e.g. Very low, Low, Intermediate, High, Very high).

* **`IPSSR Cytogenetic Risk`**

  * Cytogenetic risk subcomponent of IPSS-R (e.g. Good, Intermediate, Poor, Very poor).

> These can be used as features, but be careful about leakage: they are *derived* from other variables (like cytogenetics and counts). You might decide to use them or instead build your own feature summarization. 

#### Outcome variables (targets in `train`)

* **`Leukemia-Free Survival (months)`**
  Time (months) until evolution to acute leukemia or last leukemia-free follow-up.

* **`Evolution to acute leukemia (yes=1/no=2)`**
  Binary event: 1 if patient evolved to acute leukemia; 2 otherwise.

* **`Overall Survival (months)`**
  Time from diagnosis to death or last follow-up.

  > This is exactly the **`survival_month`** target you will predict on test.

* **`Dead/Alive at last follow-up (alive=1,dead=2)`**
  Vital status at last follow-up:

  * 1 = alive
  * 2 = dead

> These outcome variables are used **only in the training set** to fit your models. In the test set they are not available / masked.

### 3.4 Cytogenetic Summary Features

Features like:

* `del5q`
* `Lossofchr5ordel5qPLUSother`
* `Lossofchr7ordel7q`
* `Gainofchr8`
* `Lossofchr9ordel9q`
* `Isochr17qort17p`
* `Lossofchr20ordel20q`
* `LossofchrY`
* â€¦and similar.

All of these are **binary indicators** representing whether a certain chromosomal abnormality is present.

Pattern:

* **`del5q`** â†’ deletion of long arm of chromosome 5
* **`Lossofchr7ordel7q`** â†’ loss of chromosome 7 or deletion of its long arm
* **`Gainofchr8`** â†’ additional copy of chromosome 8
* etc.

You can use these directly, or **aggregate** them into higher-level features (e.g. count of poor-risk cytogenetic events).

### 3.5 Detailed Cytogenetic Features

A large block of columns like:

* `del7`, `del7p`, `del3p`, `del16q`, `del1p`, `del21`, `del8q`, `delXp`, `delXq`, `del19q`, `del20`, `del20p`, `del4q`, etc.
* `plus1`, `plus1p`, `plus3p`, `plus3q`, `plus8q`, `plus20q`, `plusX`, `plusY`, `plus12`, `plus12q`, `plus17q`, `plus19q`, etc.

General rule:

* Columns starting with **`del`** â†’ indicate **loss or deletion** of that chromosome/arm/segment

  * Example: `del7p` = deletion of the short arm of chromosome 7

* Columns starting with **`plus`** â†’ indicate **gain** of that chromosome/arm/segment

  * Example: `plus20q` = gain of the long arm of chromosome 20

All are binary. This part of the dataset is intentionally **rich and somewhat redundant**, giving opportunities for:

* Feature grouping (e.g. number of abnormalities per chromosome)
* Feature selection
* Domain-informed aggregation (e.g. total number of high-risk lesions)

### 3.6 Gene Mutation Features

A long list of gene columns, for example:

`TET2`, `ASXL1`, `SF3B1`, `SRSF2`, `DNMT3A`, `RUNX1`, `TP53`, `STAG2`, `U2AF1`, `EZH2`, `BCOR`, `CBL`, `ZRSR2`, `NRAS`, `IDH2`, `CUX1`, `NF1`, `KRAS`, `SETBP1`, `DDX41`, `PHF6`, `JAK2`, `MLL_PTD`, `IDH1`, `PTPN11`, `ETV6`, `ETNK1`, `CEBPA`, `MPL`, `SH2B3`, `PPM1D`, `BRCC3`, `KMT2C`, `NPM1`, `BCORL1`, `GATA2`, `WT1`, `CTCF`, `ZBTB33`, `EP300`, `GNB1`, `CSNK1A1`, `ARID2`, `PRPF8`, `ASXL2`, `GNAS`, `U2AF2`, `KMT2D`, `KIT`, `NFE2`, `RAD21`, `CREBBP`, `SMC1A`, `KDM6A`, `CSF3R`, `DDX54`, `FLT3`, `FLT3_ITD`, `MGA`, `LUC7L2`, `SUZ12`, `BRAF`, `EED`, `IRF1`, `ATRX`, `DDX4`, `STAT3`, `RAD50`, `CALR`, `ROBO2`, `SMC3`, `DDX23`, `ROBO1`, `SETD2`, `TERT`, `DHX33`, `GATA1`, `MYC`, `PIK3CA`, `RB1`, `RRAS`, `CDKN2A`, `EGFR`, `CSF1R`, `DNMT3B`, `ZMYM3`, `RAC1`, `STAT5A`, `ARID1A`, `CHEK2`, `MLL`, `SPRED2`, `KDM5C`, `NOTCH2`, `CDKN1B`, `DICER1`, `NIPBL`, `SAMHD1`, `SMG1`, `FAM175A`, `H3F3A`, `PAPD5`, `SF1`, `SRCAP`, `STAG1`, `WHSC1`, `ABL1`, `CDKN2B`, `HIPK2`, `NOTCH1`, `PAX5`, `PTEN`, `BAP1`, `BCL10`, `CDK4`, `CDKN2C`, `IRF4`, `IRF8`, `JAK3`, `JARID2`, `NF2`, `NXF1`, `PRPF40A`, `PTPRF`, `RBBP4`, `ZNF318`, â€¦

General rule:

* Each gene column is typically a **binary indicator**:

  * 0 = **no detected mutation**
  * 1 = **mutation present**

* Special case examples:

  * `MLL_PTD` â†’ partial tandem duplication of MLL (KMT2A)
  * `FLT3_ITD` â†’ internal tandem duplication in FLT3

You can:

* Use them individually (e.g. TP53 mutation)
* Aggregate (e.g. total number of mutations per patient, number of splicing-factor mutations, aggregate mutations that together point towards the same disease's phenotype etc.)
* Apply feature selection to reduce dimensionality

---

## 4. Prediction Targets & Task

### 4.1 Targets to Predict (on Test Set)

Your job is to predict **two outcomes for each patient in the test set**:

1. **`survival_month`**

   * Predicted overall survival time in months

2. **`alive_dead_status`**

   * Binary vital status at last follow-up:

     * **1 = alive**
     * **2 = dead**

These must be reported in the **submission Excel** alongside the corresponding `PatientID`.

### 4.2 Relationship to Training Outcomes

In the `train` dataset, you have outcome variables including:

* `Overall Survival (months)` â†’ that in test is `survival_month`
* `Dead/Alive at last follow-up (alive=1,dead=2)` â†’ same  as `alive_dead_status`

HINT: You can use these to perform 2 tasks separately:

* Train a **regression or survival model** for `Overall Survival (months)`
* Train a **classification model** for `Dead/Alive at last follow-up`

Then apply those models to the test set to generate predictions for:

* `survival_month`
* `alive_dead_status`

You may choose to:

* Model both tasks independently, or
* Use a combined strategy (e.g. survival modeling plus thresholding)

---

## 5. Evaluation & Scoring

### 5.1 Primary Metrics

The **primary **quantitative evaluation** on the test set will be based on:

* **Accuracy** (on the **binary** `alive_dead_status` prediction)
* **Matthews Correlation Coefficient (MCC)** (also on `alive_dead_status`)
* **Mean Absolute Error (MAE)** on the **continuous** `survival_month` prediction  
  â†’ average absolute difference (in months) between predicted and true survival time (**lower is better**).

Models will be **ranked** according to these performance measures (higher is better).

### 5.2 Ranking and Points

For **N teams** (e.g., **14 teams**):

* The team with the **best performance** (highest rank) receives **N points**
* The second-best team receives **Nâˆ’1 points**
* â€¦
* The last team receives **1 point**

For example, with 14 teams:

* 1st place â†’ 14 points
* 2nd place â†’ 13 points
* â€¦
* 14th place â†’ 1 point

These **â€œmetric pointsâ€** are the base for the final score.

### 5.3 Penalty for Data Leakage

Any kind of **data leakage** (using information from the test set in a way that artificially boosts performance, or misusing target variables) will be penalized with:

* **âˆ’5 points**

Examples of leakage (to avoid):

* Training or tuning on test labels (if you somehow obtain them)
* Using row-wise matching tricks (e.g., ordering artifacts) that exploit how data is constructed rather than true prediction
* Leaking target information into features through faulty preprocessing

When in doubt, keep your pipeline **honest and clean** (train/validation/test separation).

### 5.4 Jury Panel Scoring

After the quantitative metrics, a **jury panel** will evaluate the **presented workflow** for each team using the following criteria:

1. **Originality** (0â€“5 points)

   * Creativity of the approach
   * Novel combinations of techniques
   * Interesting domain-informed ideas

2. **Completeness** (0â€“4 points)

   * How well the team covered the end-to-end pipeline: EDA, modeling, evaluation, validation, explainability, and interpretation

3. **Rigor** (0â€“5 points)

   * Quality of the rationale behind choices
   * Sound methodology and evaluation (e.g. proper cross-validation, reasonable baselines)

4. **Explainability** (0â€“3 points)

   * How well the model and its predictions are explained
   * Use of tools like feature importance, SHAP, etc.

5. **Bonus points** (0â€“5 points)

   * Additional â€œX factorâ€ as judged by the panel
   * Could include teamwork, clarity, medical insight, technical elegance, etc.

> The final ranking will combine **metric-based points** and **jury-based points**, minus any penalties for data leakage.

---

## 6. Rules & Logistics

### 6.1 Deadline

* All work must be completed and submitted **by 17:45** on challenge day.
* Late submissions may not be considered in the final ranking.

### 6.2 Allowed Resources

Participants are **explicitly allowed** to use:

* Any **online resources** (documentation, blogs, papers, etc.)
* Any **generative AI model** (including large language models) to assist with:

  * Coding
  * Documentation
  * Ideas for feature engineering or modeling
  * Explaining results

> However, the **responsibility** for understanding and validating the outputs remains with the team. The jury will look for genuine understanding, not blind use of tools.

### 6.3 Collaboration

* Teams are intentionally composed to mix **medical knowledge** and **technical/ML knowledge**.
* **Internal team collaboration** is essential.
* The dataset includes **redundant information** (e.g. multiple cytogenetic encodings, overlapping clinical and risk features) to encourage:

  * Domain-based simplification
  * Feature engineering
  * Sensible feature selection

If, for any reason (e.g. missing people), a team ends up with fewer members than initially planned, **organizers may:**

* Reallocate members to other teams, or
* Merge smaller teams together

### 6.4 Feature Engineering & Explainability (Strongly Encouraged)

You are **strongly encouraged** to:

* Create **artificial features** (e.g. aggregated cytogenetic scores, mutational burden)
* Perform **feature selection** (technical or domain-driven)
* Perform **feature aggregation** (grouping variables by pathway, chromosome, or domain meaning)
* Include **explainability techniques** in your analysis and presentation

Explainability is not only useful for the jury but also for validating that your model makes sense clinically.

---

## 7. Deliverables & Submission

Each team must submit **two things**:

1. **Excel file with predictions named as follows: teamN.xlsx [eg., for Team 1 it will be team1.xlsx**
2. **PPTX presentation** (3â€“5 minutes per team), named as specified in point 1 for the Excel file

### 7.1 Prediction File (Excel)

Use the provided **Excel template** in the `deliverable/` folder.

Required columns:

| Column              | Description                                         |
| ------------------- | --------------------------------------------------- |
| `PatientID`         | Exactly the patient IDs from the **test** data      |
| `survival_month`    | Predicted overall survival (months)                 |
| `alive_dead_status` | Predicted vital status: **1 = alive**, **2 = dead** |

**Important rules:**

* Do **not** modify the column names.
* Do **not** add extra columns.
* Make sure:

  * All `PatientID`s from test are included.
  * No duplicates.
  * No missing predictions.

### 7.2 Presentation (PPTX)

Use the **template** in `deliverable/`:

* Duration: **3â€“5 minutes per team**


Try to keep slides clear, with visuals where useful (plots, tables, diagrams).

### 7.3 How to Submit

Send an **email** with both files attached:

* **To**: `humanitasuniversityai@gmail.com`
* **Subject**: your **TEAM number only** (e.g. `TEAM 4`)

Attachments:

* Your **Excel file** with predictions (named teamN.xslx, where N is your team's number)
* Your **PPTX** presentation (named teamN.pptx, where N is your team's number)

> Make sure to send your email **before 17:45 CET**.

---

## 8. Teams

Below is the list of teams and members (by surname):

* **TEAM 1**

  * Betzu
  * Ciavattini
  * Bordin
  * Pagano

* **TEAM 2**

  * Scarpanti
  * Lurani Cernuschi
  * CracÃ²
  * Dejeu

* **TEAM 3**

  * Menga
  * Dâ€™Amico
  * Caporali
  * Jafour

* **TEAM 4**

  * Barnaba
  * Canonico
  * Mally
  * Ghazy

* **TEAM 5**

  * Fogliati
  * Castelli
  * Di Fabio
  * Guarnerio

* **TEAM 6**

  * Basoz
  * Barone
  * Morar
  * Singh

* **TEAM 7**

  * Trasolini
  * El Hidraoui
  * Marandici
  * Yadav

* **TEAM 8**

  * Buonamassa
  * Conti
  * Dâ€™Onofrio
  * Singh

* **TEAM 9**

  * Pelosi
  * Bayraktar
  * Russo
  * Mihaila

* **TEAM 10**

  * Duggento
  * Jafour
  * Belet
  * Mollo

* **TEAM 11**

  * Biancardi
  * Cao
  * Royai
  * Vento

* **TEAM 12**

  * Massolini
  * de Spuches
  * Casamassima
  * Colpani

* **TEAM 13**

  * Cappelletti
  * Basone
  * Amato
  * Creta

* **TEAM 14**

  * Godinez
  * Sakaoglu
  * Costa
  * Lakasz

> If a team ends up with fewer members than expected, organizers may reallocate or merge teams.

---

## 9. Contact

For any question or to submit your final deliverables come to us.

Please include your **TEAM number** clearly in the **email subject**.

---

Good luck, have fun, and make the most of the synergy between **clinical insight** and **data science** ðŸš€

```
```
